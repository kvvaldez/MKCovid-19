{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "batch_size_Bayes Optimization and Transfer Learning for covid.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_VQl-EYQ7k8",
        "outputId": "4162af8f-1d43-4b58-890d-e0d0c2efe369"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import easydict\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import copy, pickle, os, time\n",
        "import argparse\n",
        "\n",
        "from hyperopt import hp\n",
        "from hyperopt.pyll.stochastic import sample\n",
        "from hyperopt import tpe\n",
        "from hyperopt import Trials\n",
        "from hyperopt import fmin\n",
        "import csv\n",
        "from hyperopt import STATUS_OK\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#acces to my drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYGvgtoKL20P"
      },
      "source": [
        "# Define the search space\n",
        "space = {\n",
        "    'pretrained': hp.choice('pretrained',['resnet18','resnet50', 'googlenet', 'vgg16','squeezenet','densenet']),\n",
        "    'optimizer': hp.choice('optimizer',['SGD','Adam','RMSprop','Adagrad','Adadelta','Adamax']),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
        "    'batch_size': hp.quniform('batch_size', 5, 100, 5),\n",
        "    'momentum': hp.quniform('momentum', 0, 1, 0.1),\n",
        "}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejomTilV4iCu"
      },
      "source": [
        "# Define some parameters and paths\n",
        "args = easydict.EasyDict(\n",
        "    {\n",
        "    \"epochs\": 1,\n",
        "    \"num_workers\": 6,\n",
        "    \"num_class\": 2,\n",
        "    \"max_evals\": 2,\n",
        "    \"original_dataset_path\": '/content/drive/MyDrive/Articulo/dataset5k/originalData/',\n",
        "    \"selected_dataset_path\": '/content/drive/MyDrive/Articulo/dataset10k/selectData/',\n",
        "    \"test_dataset_path\": '/content/drive/MyDrive/Articulo/dataset5k/testData/',\n",
        "    \"path_model\" : '/content/drive/MyDrive/Miguel UNSA/',\n",
        "    \"name_csv\": 'gbm_trials_batch.csv',\n",
        "    \"name_hyperopt\": 'my_model_batch.hyperopt',\n",
        "    })"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6p1ygMiL2_-",
        "outputId": "8d95790a-839e-46dc-dfc9-231a02f219ab"
      },
      "source": [
        "sample(space)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 25.0,\n",
              " 'learning_rate': 0.037402977848495736,\n",
              " 'momentum': 1.0,\n",
              " 'optimizer': 'RMSprop',\n",
              " 'pretrained': 'resnet18'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ6kn5kURA3t"
      },
      "source": [
        "# Define pre-processing data\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61P9jrEHkGNT"
      },
      "source": [
        "# Preparate data in pytorch format\n",
        "def process_standard_data(batch_size):\n",
        "\n",
        "  image_datasets = {x: datasets.ImageFolder(os.path.join(args.selected_dataset_path, x),\n",
        "                                                data_transforms[x])\n",
        "                        for x in ['train', 'val']}\n",
        "\n",
        "  dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size= batch_size,\n",
        "                                                  shuffle=True, num_workers= args.num_workers)\n",
        "                    for x in ['train', 'val']}\n",
        "\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "  class_names = image_datasets['train'].classes  ## 0: non-covid, and 1: covid-19\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "  return dataloaders, dataset_sizes, device"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl98wjBARXMf"
      },
      "source": [
        "# Train model using pre-trained model\n",
        "def train_model(model, criterion, optimizer, scheduler, batch_szie, num_epochs, dataloaders, dataset_sizes, device):\n",
        "    since = time.time()\n",
        "   \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    train_acc= list()\n",
        "    valid_acc= list()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            running_prec= 0.0\n",
        "            running_rec = 0.0\n",
        "            running_f1  = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            cur_batch_ind= 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                cur_acc= torch.sum(preds == labels.data).double()/batch_szie\n",
        "                cur_batch_ind +=1\n",
        "                \n",
        "                if phase=='train':\n",
        "                    train_acc.append(cur_acc)\n",
        "                else:\n",
        "                    valid_acc.append(cur_acc)\n",
        "                \n",
        "            epoch_loss= running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]            \n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_epoch= epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc= %.3f at Epoch: %d' %(best_acc,best_epoch) )\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, best_acc"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQWOlcm5QH41"
      },
      "source": [
        "def set_requires_grad(model_conv):\n",
        "  for param in model_conv.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hICDQjeOeTS"
      },
      "source": [
        "# Configure the last layer for Resnet18 model, Resnet50 model, and Googlenet model.\n",
        "def configure_lastlayer_mix(model_conv):\n",
        "\n",
        "  set_requires_grad(model_conv)\n",
        "  # Parameters of newly constructed modules have requires_grad=True by default     \n",
        "  num_ftrs = model_conv.fc.in_features        \n",
        "  model_conv.fc = nn.Linear(num_ftrs, args.num_class)\n",
        "\n",
        "  return model_conv, model_conv.fc"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5edzCTbrQMCW"
      },
      "source": [
        "# Configure the last layer for vgg16 model.\n",
        "def configure_lastlayer_vgg16(model_conv):\n",
        "\n",
        "  set_requires_grad(model_conv) \n",
        "  num_ftrs = model_conv.classifier[3].in_features \n",
        "  features = list(model_conv.classifier.children())[:-1] # Remove last layer\n",
        "  features.extend([nn.Linear(num_ftrs, args.num_class)]) # Add our layer with 4 outputs\n",
        "  model_conv.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "  return model_conv, model_conv.classifier"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4sHbzXwH1XE"
      },
      "source": [
        "# Configure the last layer for Squeezenet model.\n",
        "def configure_lastlayer_squeezenet(model_conv):\n",
        "\n",
        "  set_requires_grad(model_conv)\n",
        "  model_conv.classifier[1] = nn.Conv2d(512, args.num_class, kernel_size=(1,1), stride=(1,1))\n",
        "  model_conv.num_classes = args.num_class\n",
        "\n",
        "  return model_conv"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBRPZNs2Rf3e"
      },
      "source": [
        "# Configure the last layer for Densenet model.\n",
        "def configure_lastlayer_densenet(model_conv):\n",
        "\n",
        "  set_requires_grad(model_conv)    \n",
        "  num_ftrs = model_conv.classifier.in_features        \n",
        "  model_conv.classifier = nn.Linear(num_ftrs, args.num_class)\n",
        "\n",
        "  return model_conv, model_conv.classifier"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GTwjk32RacV"
      },
      "source": [
        "# Load pre-trained models and configure last layer\n",
        "def return_pre_training_model(learning_rate, momentum, pretrained, optimizer, device, pretrained_bool ):\n",
        "\n",
        "  if pretrained == 'resnet18':\n",
        "    model_conv = torchvision.models.resnet18(pretrained=pretrained_bool)\n",
        "    model_conv, model_fc = configure_lastlayer_mix(model_conv)\n",
        "\n",
        "  elif pretrained == 'resnet50':\n",
        "    model_conv = torchvision.models.resnet50(pretrained=pretrained_bool)\n",
        "    model_conv, model_fc = configure_lastlayer_mix(model_conv)\n",
        "\n",
        "  elif pretrained == 'googlenet':\n",
        "    model_conv = torchvision.models.googlenet(pretrained=pretrained_bool)\n",
        "    model_conv, model_fc = configure_lastlayer_mix(model_conv)\n",
        "\n",
        "  elif pretrained == 'vgg16':\n",
        "    model_conv = torchvision.models.vgg16(pretrained=pretrained_bool)\n",
        "    model_conv, model_fc = configure_lastlayer_vgg16(model_conv)\n",
        "\n",
        "  elif pretrained == 'squeezenet':\n",
        "    model_conv = torchvision.models.squeezenet1_0(pretrained=pretrained_bool)\n",
        "    model_conv = configure_lastlayer_squeezenet(model_conv)\n",
        "    model_fc = model_conv\n",
        "\n",
        "  elif pretrained == 'densenet':\n",
        "    model_conv =torchvision.models.densenet121(pretrained=pretrained_bool)\n",
        "    model_conv, model_fc = configure_lastlayer_densenet(model_conv)\n",
        "\n",
        "  # Use GPU or CPU\n",
        "  model_conv = model_conv.to(device)  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Observe that only parameters of final layer are being optimized as\n",
        "  # opoosed to before.\n",
        "  \n",
        "  if optimizer == 'SGD':\n",
        "    optimizer_conv = optim.SGD(model_fc.parameters(), lr= learning_rate, momentum= momentum)\n",
        "\n",
        "  elif optimizer == 'Adam':\n",
        "    optimizer_conv = optim.Adam(model_fc.parameters(), lr= learning_rate)\n",
        "\n",
        "  elif optimizer == 'RMSprop':\n",
        "    optimizer_conv = optim.RMSprop(model_fc.parameters(), lr= learning_rate, momentum= momentum)\n",
        "\n",
        "  elif optimizer == 'Adagrad':\n",
        "    optimizer_conv = optim.Adagrad(model_fc.parameters(), lr= learning_rate)\n",
        "\n",
        "  elif optimizer == 'Adadelta':\n",
        "    optimizer_conv = optim.Adadelta(model_fc.parameters(), lr= learning_rate)\n",
        "    \n",
        "  elif optimizer == 'Adamax':\n",
        "    optimizer_conv = optim.Adamax(model_fc.parameters(), lr= learning_rate)\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "\n",
        "  return model_conv, criterion, optimizer_conv, exp_lr_scheduler"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoGChARpRc5u"
      },
      "source": [
        "# Function that train and get the best model of each iterations\n",
        "def evaluation_model(model_conv_pretrain, criterion, optimizer_conv, exp_lr_scheduler, dataloaders, dataset_sizes, batch_size, device):\n",
        "  \n",
        "  model_conv, valid_acc = train_model(model_conv_pretrain, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, batch_size, args.epochs, dataloaders, dataset_sizes, device)\n",
        "  model_conv.eval()\n",
        "  #torch.save(model_conv, args.path_model+'/covid_sort2_resnet18_epoch%d.pt' %args.epochs )\n",
        "  return valid_acc"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzMODBrUNlgB"
      },
      "source": [
        "# Defines the objective function for loss minimization\n",
        "def objective(params):\n",
        "    \"\"\"Objective function for classifier  Hyperparameter Optimization\"\"\"\n",
        "\n",
        "    print(params)\n",
        "    # Keep track of evals\n",
        "    global ITERATION\n",
        "    \n",
        "    ITERATION += 1\n",
        "\n",
        "\n",
        "    start = timer()\n",
        "    \n",
        "    dataloaders_, dataset_sizes_, device_ = process_standard_data(int(params['batch_size']))\n",
        "    model_conv_pretrain, criterion, optimizer_conv, exp_lr_scheduler = return_pre_training_model(params['learning_rate'], params['momentum'], params['pretrained'], params['optimizer'], device_, True)\n",
        "    best_score = evaluation_model(model_conv_pretrain, criterion, optimizer_conv, exp_lr_scheduler,  dataloaders_, dataset_sizes_, int(params['batch_size']), device_)\n",
        "    best_score = float(best_score.cpu().numpy()) #cuando se usa Gpu convertir tensor a .cpu()\n",
        "\n",
        "    run_time = timer() - start\n",
        "    \n",
        "    # Loss must be minimized\n",
        "    loss = 1 - best_score\n",
        "\n",
        "    # Write to the csv file ('a' means append)\n",
        "    of_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(of_connection)\n",
        "    writer.writerow([loss, params, ITERATION, run_time])\n",
        "    \n",
        "    # Dictionary with information for evaluation\n",
        "    return {'loss': loss, 'params': params, 'iteration': ITERATION, \n",
        "            'train_time': run_time, 'status': STATUS_OK}"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-407dvup35vY"
      },
      "source": [
        "# Optimization algorithm\n",
        "tpe_algorithm = tpe.suggest"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgFUSJ5539_R",
        "outputId": "e1cc8814-403d-4227-c49f-a7391dd526dc"
      },
      "source": [
        "# Open or create file CVS to save values of hiperparameters\n",
        "out_file = args.path_model+args.name_csv\n",
        "if not os.path.exists(out_file):\n",
        "  print('Create new file CSV')\n",
        "  # File to save first results\n",
        "  of_connection = open(out_file, 'w')\n",
        "  writer = csv.writer(of_connection)\n",
        "  # Write the headers to the file\n",
        "  writer.writerow(['loss', 'params', 'iteration', 'train_time'])\n",
        "  of_connection.close()\n",
        "else:\n",
        "  print('CSV already exists')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CSV already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG809WxUtXiw",
        "outputId": "604ace8c-82a3-42e2-ec2c-2cc20d221349"
      },
      "source": [
        "# Try to load an already saved trials object, and increase the max\n",
        "MAX_EVALS = args.max_evals\n",
        "try:  \n",
        "  bayes_trials = pickle.load(open(args.path_model+args.name_hyperopt, \"rb\"))\n",
        "  print(\"Found saved Trials! Loading...\")\n",
        "  MAX_EVALS = len(bayes_trials.trials) + MAX_EVALS #+ trials_step\n",
        "  print(str(len(bayes_trials.trials))+\" iterations have already been run\")\n",
        "except:  # create a new trials object and start searching\n",
        "  bayes_trials = Trials()\n",
        "  print('Create new file hyperopt')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found saved Trials! Loading...\n",
            "2 iterations have already been run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iys_KvZn4NV-",
        "outputId": "21db783d-d769-47c4-c8ff-ffa1c82e98cb"
      },
      "source": [
        "# Execute bay optimization, using the function for minimization\n",
        "\n",
        "#%%capture\n",
        "\n",
        "# Global variable\n",
        "global  ITERATION\n",
        "\n",
        "ITERATION = len(bayes_trials.trials)\n",
        "\n",
        "# Run optimization\n",
        "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
        "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))\n",
        "\n",
        "print(\"Best:\", best)\n",
        "    \n",
        "# save the trials object\n",
        "with open(args.path_model+args.name_hyperopt, \"wb\") as f:\n",
        "  pickle.dump(bayes_trials, f)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 5.0, 'learning_rate': 0.040518777517780005, 'momentum': 0.2, 'optimizer': 'Adamax', 'pretrained': 'googlenet'}\n",
            "  0%|          | 0/2 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "----------\n",
            "  0%|          | 0/2 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete in 5m 18s\n",
            "Best val Acc= 0.895 at Epoch: 0\n",
            "{'batch_size': 15.0, 'learning_rate': 0.016149847971048317, 'momentum': 0.2, 'optimizer': 'Adam', 'pretrained': 'resnet18'}\n",
            "Epoch 1/1\n",
            "----------\n",
            "Training complete in 3m 49s\n",
            "Best val Acc= 0.929 at Epoch: 0\n",
            "100%|██████████| 2/2 [09:07<00:00, 273.93s/it, best loss: 0.0707692307692308]\n",
            "Best: {'batch_size': 15.0, 'learning_rate': 0.016149847971048317, 'momentum': 0.2, 'optimizer': 1, 'pretrained': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m_6Gdyj5WrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9a2089-139e-4f8e-fa48-7c494696c767"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 15670998718007900824]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yirwNJlUbHj"
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": []
    }
  ]
}